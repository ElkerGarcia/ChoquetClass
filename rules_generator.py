# -*- coding: utf-8 -*-
"""Rules_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18H0hix0s9iqX9WAELQrPKAKQcqhTrjw1
"""

import random
from . import conjuntos_fuzzy as cf
from . import variables_linguisticas as vl
from . import var_linguistic_predef as vlp
import numpy as np
from copy import deepcopy
import pandas as pd



class RuleGenerator:
    def __init__(self, data, attributes, linguistic_variables, N=5, H=None):
        """
        linguistic_variables: diccionario con variables lingüísticas por atributo
        """
        self.data = data
        self.attributes = attributes
        self.linguistic_variables = linguistic_variables  # ahora es un dict
        self.N = N
        self.H = H if H is not None else len(data)

    def min_membership(self, atributo, col_clase, subset=None, clase_fija=None):
        if atributo not in self.data.columns:
            raise ValueError(f"El atributo '{atributo}' no está en el DataFrame.")
        if col_clase not in self.data.columns:
            raise ValueError(f"La columna de clase '{col_clase}' no está en el DataFrame.")

        # Si no se pasa subset, seleccionamos uno nuevo (solo la primera vez)
        if subset is None:
            clases_disponibles = self.data[col_clase].unique()
            clase_seleccionada = random.choice(clases_disponibles) if clase_fija is None else clase_fija
            subset_clase = self.data[self.data[col_clase] == clase_seleccionada]

            if len(subset_clase) < self.H:
                raise ValueError(f"No hay suficientes ejemplos de la clase '{clase_seleccionada}' (se requieren {self.H}).")

            subset = subset_clase.sample(n=self.H, random_state=random.randint(0, 9999))
        else:
            clase_seleccionada = clase_fija  # usar la clase ya fijada

        fuzzy_min = {}
        for fs in self.linguistic_variables[atributo]._FSlist:
            term = fs._term
            min_val = min(fs.get_value(valor) for valor in subset[atributo])
            fuzzy_min[term] = min_val

        return clase_seleccionada, fuzzy_min, subset

    def grado_membresia_modificada(self, fuzzy_min):
        return {term: valor if valor >= 0.5 else 0 for term, valor in fuzzy_min.items()}

    def probability_selection(self, grado):
        total = sum(grado.values())
        if total == 0:
            return {term: "don't care" for term in grado}
        else:
            return {term: valor / total for term, valor in grado.items()}


    def generar_regla_nocompacta(self, col_clase):
        regla = {}
        antecedent = {}

        # Seleccionamos subset y clase usando el primer atributo
        primer_atributo = self.attributes[0]
        clase_seleccionada, fuzzy_min, subset = self.min_membership(primer_atributo, col_clase)

        # Procesar el primer atributo
        grado = self.grado_membresia_modificada(fuzzy_min)
        probabilidad = self.probability_selection(grado)
        antecedent[primer_atributo] = grado
        regla[primer_atributo] = probabilidad

        # Procesar los demás atributos usando el mismo subset y clase fija
        for atributo in self.attributes[1:]:
            _, fuzzy_min_attr, _ = self.min_membership(atributo, col_clase, subset=subset, clase_fija=clase_seleccionada)
            grado = self.grado_membresia_modificada(fuzzy_min_attr)
            probabilidad = self.probability_selection(grado)
            antecedent[atributo] = grado
            regla[atributo] = probabilidad

        # Agregar consecuente fijo
        regla['consecuente'] = clase_seleccionada

        return regla, subset


    def generar_regla(self, col_clase):
        regla = {}
        antecedent = {}

        # Seleccionamos subset y clase usando el primer atributo
        primer_atributo = self.attributes[0]
        clase_seleccionada, fuzzy_min, subset = self.min_membership(primer_atributo, col_clase)

        # Procesar el primer atributo
        grado = self.grado_membresia_modificada(fuzzy_min)
        probabilidad = self.probability_selection(grado)

        # Guardar solo los términos con valor > 0
        for termino, valor in grado.items():
            if valor > 0:
                antecedent[primer_atributo] = termino
                regla[primer_atributo] = termino

        # Procesar los demás atributos usando el mismo subset y clase fija
        for atributo in self.attributes[1:]:
            _, fuzzy_min_attr, _ = self.min_membership(atributo, col_clase, subset=subset, clase_fija=clase_seleccionada)
            grado = self.grado_membresia_modificada(fuzzy_min_attr)
            probabilidad = self.probability_selection(grado)

            for termino, valor in grado.items():
                if valor > 0:
                    antecedent[atributo] = termino
                    regla[atributo] = termino

        # Agregar consecuente fijo
        regla['consecuente'] = clase_seleccionada

        return regla, subset




    def generar_reglas(self, col_clase):
        reglas = []
        for _ in range(self.N):
            regla, subset = self.generar_regla(col_clase)
            reglas.append(regla)
        return reglas




class GradoPertenencia:
    def __init__(self, variables):
        self.variables = variables

    def calcular(self, regla, ejemplo):
        """
        Calcula el grado de pertenencia de un ejemplo con respecto a una regla.
        """
        grados = []
        for atributo, termino in regla.items():
            if atributo == "consecuente":  
                continue
            valor = ejemplo[atributo]  # valor numérico
            valores_membresia = self.variables[atributo].get_values(valor)
            mu = valores_membresia[termino]
            grados.append(mu)

        return min(grados) if grados else 0.0

    def calcular_varias(self, reglas, ejemplo):
        """
        Calcula los grados de pertenencia de un ejemplo con respecto a varias reglas.
        
        reglas: lista de diccionarios
        ejemplo: fila del DataFrame
        """
        resultados = []
        for i, regla in enumerate(reglas, start=1):
            gp = self.calcular(regla, ejemplo)
            resultados.append({"regla": i, "grado": gp, "consecuente": regla["consecuente"]})
        return resultados





# Con esta clase podemos unir las reglas individuales de la forma IF-THEN creadas por el usuario
class Rule:
    def __init__(self):
        self.antecedente = []
        self.consecuente = None

    def add_condition(self, variable, label):
        self.antecedente.append((variable, label))

    def set_consequent(self, clase):
        self.consecuente = clase

    def __repr__(self):
        condiciones = " AND ".join([f"{v} IS {l}" for v, l in self.antecedente])
        return f"IF {condiciones} THEN {self.consecuente}"


# --- Builder fluido para sintaxis tipo lenguaje natural ---
class IF:
    def __init__(self, variable):
        self.rule = Rule()
        self.current_var = variable

    def IS(self, label):
        self.rule.add_condition(self.current_var, label)
        return _RuleBuilder(self.rule)


class _RuleBuilder:
    def __init__(self, rule):
        self.rule = rule

    def AND(self, variable):
        self.current_var = variable
        return _AndHelper(self.rule, self.current_var)

    def THEN(self, clase):
        self.rule.set_consequent(clase)
        return self.rule


class _AndHelper:
    def __init__(self, rule, variable):
        self.rule = rule
        self.current_var = variable

    def IS(self, label):
        self.rule.add_condition(self.current_var, label)
        return _RuleBuilder(self.rule)


# --- Clase Base de Reglas ---
class RuleBase:
    def __init__(self):
        self.rules = []

    def add_rules(self, rules_list):
        for rule in rules_list:
            if isinstance(rule, Rule):
                # Convertir automáticamente cada regla en un diccionario
                dic = {var: lbl for var, lbl in rule.antecedente}
                dic["consecuente"] = rule.consecuente
                self.rules.append(dic)
            else:
                raise TypeError("Solo se pueden agregar objetos de tipo Rule")
        return self  # para poder encadenar métodos

    def show(self):
        for r in self.rules:
            print(r)





#calcula el peso de cada regla con el PCF
class PesoReglaPCF:
    def __init__(self, variables):
        self.gp = GradoPertenencia(variables)  # usamos tu clase anterior

    def calcular_peso(self, regla, data, col_clase):
        """
        Calcula el peso de una regla con PCF.

        regla: diccionario de la forma:
            {'atributo1': 'MEDIO', ..., 'consecuente': 'Gentoo'}

        data: DataFrame con los ejemplos
        col_clase: nombre de la columna de clase en data
        """
        clase_regla = regla["consecuente"]

        suma_clase = 0.0
        suma_no_clase = 0.0
        suma_total = 0.0

        for idx, ejemplo in data.iterrows():
            mu = self.gp.calcular(regla, ejemplo)
            suma_total += mu

            if ejemplo[col_clase] == clase_regla:
                suma_clase += mu
            else:
                suma_no_clase += mu

        if suma_total == 0:
            return 0.0  # para evitar división por cero

        peso = (suma_clase - suma_no_clase) / suma_total
        return peso





class Asociacion_fuzzy:
    def __init__(self, variables):
        self.gp = GradoPertenencia(variables)
        self.pcf = PesoReglaPCF(variables)

    def calcular(self, reglas_f, ejemplo, data, col_clase):
        """
        Calcula el grado de asociación de un ejemplo con respecto a una o varias reglas.
        - reglas_f: dict (una sola regla) o list[dict] (varias reglas)
        - ejemplo: fila de datos (pandas.Series)
        - data: DataFrame con todos los datos
        - col_clase: columna con la clase objetivo
        """
        # Caso 1: una sola regla
        if isinstance(reglas_f, dict):
            mu = self.gp.calcular(reglas_f, ejemplo)
            peso = self.pcf.calcular_peso(reglas_f, data, col_clase)
            clase = reglas_f.get("consecuente", None)
            return {clase: mu * peso}

        # Caso 2: lista de reglas
        elif isinstance(reglas_f, list):
            asociaciones = []
            for regla_f in reglas_f:
                if not isinstance(regla_f, dict):
                    raise ValueError("Cada elemento de la lista debe ser un diccionario (una regla).")
                mu = self.gp.calcular(regla_f, ejemplo)
                peso = self.pcf.calcular_peso(regla_f, data, col_clase)
                clase = regla_f.get("consecuente", None)
                asociaciones.append({clase: mu * peso})
            return asociaciones

        else:
            raise TypeError("El parámetro 'reglas_f' debe ser un dict o una lista de dicts.")






def choquet_integral(values, q: float):
    """
    values: lista de b_j^k > 0 (asociaciones positivas) para una clase k.
    """
    if not values:
        return 0.0
    xs = sorted(float(v) for v in values)  # x_(1) <= ... <= x_(n)
    n = len(xs)
    total = 0.0
    prev = 0.0  # x_(0) = 0
    for i, x in enumerate(xs, start=1):        # i = 1..n
        delta = x - prev                        # (x_(i) - x_(i-1))
        card = n - i + 1                        # |A_(i)|
        mA = (card / n) ** q                    # m(A_(i))
        total += delta * mA
        prev = x
    return total




class SolidezChoquet:
    def __init__(self, variables):
        self.gp = GradoPertenencia(variables)
        self.pcf = PesoReglaPCF(variables)

    def asociacion(self, regla, ejemplo, data, col_clase):
        mu = self.gp.calcular(regla, ejemplo)
        rw = self.pcf.calcular_peso(regla, data, col_clase)
        return mu * rw

    def solidez_con_choquet(self, ejemplo, reglas, data, col_clase, q_vector):
        """
        Calcula la solidez usando la integral de Choquet con un vector q
        q_vector = [q1, q2, q3, ...] (uno por cada clase)
        """
        asociaciones_por_clase = {}
        clases_unicas = list(data[col_clase].unique())

        # Agrupar asociaciones por clase
        for regla in reglas:
            clase = regla["consecuente"]
            valor = self.asociacion(regla, ejemplo, data, col_clase)
            if valor > 0:
                asociaciones_por_clase.setdefault(clase, []).append(valor)

        # Aplicar Choquet para cada clase con su q correspondiente
        solidez_resultado = {}
        for idx, clase in enumerate(clases_unicas):
            valores = asociaciones_por_clase.get(clase, [])
            q = q_vector[idx]
            solidez_resultado[clase] = choquet_integral(valores, q)

        return solidez_resultado







class FitnessSimple:
    def __init__(self, variables):
        """
        Calculadora de fitness basada en el promedio de asociaciones positivas
        menos el doble del promedio de asociaciones negativas.
        """
        self.gp = GradoPertenencia(variables)
        self.pcf = PesoReglaPCF(variables)

    def asociacion(self, regla, ejemplo, data, col_clase):
        """Calcula el grado de asociación (μ * peso) de un ejemplo con una regla."""
        mu = self.gp.calcular(regla, ejemplo)
        peso = self.pcf.calcular_peso(regla, data, col_clase)
        return mu * peso

    def calcular_fitness(self, regla, data, col_clase):
        """
        Calcula el fitness de una única regla según la fórmula:
        fitness = promedio(asociaciones positivas) - 2 * promedio(asociaciones negativas)
        """
        clase_regla = regla["consecuente"]
        asociaciones_pos = []
        asociaciones_neg = []

        for _, ejemplo in data.iterrows():
            grado_asoc = self.asociacion(regla, ejemplo, data, col_clase)
            if grado_asoc > 0:
                if ejemplo[col_clase] == clase_regla:
                    asociaciones_pos.append(grado_asoc)
                else:
                    asociaciones_neg.append(grado_asoc)

        # Promedios: 0 si no hay asociaciones
        prom_pos = sum(asociaciones_pos) / len(asociaciones_pos) if asociaciones_pos else 0
        prom_neg = sum(asociaciones_neg) / len(asociaciones_neg) if asociaciones_neg else 0

        # Fitness = promedio positivo - 2 * promedio negativo
        return prom_pos - 2 * prom_neg

    def calcular_fitness_todas(self, reglas, data, col_clase):
        """
        Calcula el fitness de todas las reglas en la lista.
        Retorna un diccionario: {indice_regla: fitness}
        """
        fitness_dict = {}
        for idx, regla in enumerate(reglas):
            fitness_dict[idx] = self.calcular_fitness(regla, data, col_clase)
        return fitness_dict






def calcular_CR(test_data, reglas, solidez_model, q_vector, col_clase="class"):
    """
    Calcula la Tasa de Clasificación (CR) usando las reglas difusas y la integral de Choquet.

    Parámetros:
    - test_data: DataFrame de prueba con ejemplos a clasificar.
    - reglas: lista de reglas difusas (cada una con 'consecuente').
    - solidez_model: instancia de SolidezChoquet.
    - q_vector: lista con los valores q para cada clase en el mismo orden que aparece en test_data[col_clase].unique()
    - col_clase: nombre de la columna que contiene la clase real.

    Retorna:
    - CR: tasa de clasificación correcta (float)
    """
    total = len(test_data)
    aciertos = 0
    clases_unicas = list(test_data[col_clase].unique())

    for _, ejemplo in test_data.iterrows():
        # 1. Calcular solidez para cada clase
        solidez_por_clase = solidez_model.solidez_con_choquet(
            ejemplo=ejemplo,
            reglas=reglas,
            data=test_data,
            col_clase=col_clase,
            q_vector=q_vector
        )

        # 2. Predecir clase con mayor solidez
        clase_predicha = max(solidez_por_clase, key=solidez_por_clase.get)
        clase_real = ejemplo[col_clase]

        if clase_predicha == clase_real:
            aciertos += 1

    CR = aciertos / total
    return CR





def ajustar_q(q):
    """
    Ajusta cada valor de q según:
    - Si 0.00 < q <= 1.00 → q
    - Si 1.00 < q < 2.00 → 1 / (2 - q)
    """
    q = np.array(q, dtype=float)
    q_ajustado = np.where((q > 1.0) & (q < 2.0), 1 / (2 - q), q)
    return q_ajustado.tolist()


def optimizar_q(test_data, reglas, solidez_model, col_clase, q_inicial, num_iter=50):
    """
    Optimiza el vector q para maximizar la Tasa de Clasificación (CR)
    usando un algoritmo de búsqueda aleatoria descrito en el paper.

    Parámetros:
    - test_data: DataFrame con datos de prueba
    - reglas: lista de reglas difusas
    - solidez_model: instancia de SolidezChoquet
    - col_clase: nombre de la columna de clase
    - q_inicial: lista inicial con valores de q
    - num_iter: número de iteraciones

    Retorna:
    - q_best: vector q con mejor rendimiento
    - CR_best: tasa de clasificación asociada
    """
    clases = list(test_data[col_clase].unique())
    n_clases = len(clases)

    # Paso 4: Inicializar q_best
    q_best = q_inicial.copy()

    # Paso 5: Calcular CR inicial
    CR_best = calcular_CR(
        test_data=test_data,
        reglas=reglas,
        solidez_model=solidez_model,
        q_vector=ajustar_q(q_best),
        col_clase=col_clase
    )

    print(f"Iteración 0: CR = {CR_best:.4f}, q = {q_best}")

    # Paso 6-11: Iteraciones de optimización
    for t in range(1, num_iter + 1):
        # Paso 7: Generar q aleatorio en [0.01, 1.99]
        q_t = np.random.uniform(0.01, 1.99, size=n_clases).tolist()

        # Paso 8: Calcular CR con q_t ajustado
        CR_t = calcular_CR(
            test_data=test_data,
            reglas=reglas,
            solidez_model=solidez_model,
            q_vector=ajustar_q(q_t),
            col_clase=col_clase
        )

        # Paso 9-10: Verificar si mejora el CR
        if CR_t > CR_best:
            q_best = q_t
            CR_best = CR_t
            print(f"Iteración {t}: Nuevo mejor CR = {CR_best:.4f}, q = {q_best}")

    return ajustar_q(q_best), CR_best







# ---------- Utilidades de saneo y construcción de reglas ----------

def _terms_for_attr(variables, attr):
    return [fs._term for fs in variables[attr]._FSlist]

def ensure_valid_rule(rule, attributes, classes, variables):
    """
    Garantiza que la 'rule' tenga:
      - 'consecuente' válido
      - TODOS los atributos de 'attributes' con un término válido
    Si falta algo, se completa al azar usando 'variables'.
    """
    fixed = dict(rule) if rule is not None else {}

    # Consecuente
    if "consecuente" not in fixed or fixed["consecuente"] not in classes:
        fixed["consecuente"] = random.choice(classes)

    # Atributos
    for attr in attributes:
        if attr not in fixed:
            # escoger cualquier término válido del atributo
            fixed[attr] = random.choice(_terms_for_attr(variables, attr))
        else:
            # si el término no es válido, corrígelo
            valid_terms = set(_terms_for_attr(variables, attr))
            if fixed[attr] not in valid_terms:
                fixed[attr] = random.choice(list(valid_terms))

    return fixed

def regla_desde_ejemplo(ejemplo, variables, attributes, col_clase):
    """
    Construye una regla 'compacta' a partir de un ejemplo:
      - para cada atributo, el término con mayor pertenencia del valor del ejemplo
      - consecuente = clase del ejemplo
    """
    rule = {}
    for attr in attributes:
        valor = ejemplo[attr]
        vals = variables[attr].get_values(valor)  # dict {term: mu}
        term = max(vals, key=vals.get) if len(vals) > 0 else random.choice(_terms_for_attr(variables, attr))
        rule[attr] = term
    rule["consecuente"] = ejemplo[col_clase]
    return rule

# ---------- Operadores genéticos ----------

def crossover_reglas(parent1, parent2, cross_prob, attributes, classes, variables):
    """
    Realiza crossover entre dos reglas:
    - Mezcla atributos con probabilidad cross_prob
    - Selecciona un consecuente válido
    - Garantiza consistencia de la regla
    """
    child = {}
    for attr in attributes:
        if random.random() < cross_prob:
            child[attr] = parent1.get(attr, random.choice(_terms_for_attr(variables, attr)))
        else:
            child[attr] = parent2.get(attr, random.choice(_terms_for_attr(variables, attr)))

    # Mezcla el consecuente también
    child["consecuente"] = (
        parent1.get("consecuente", random.choice(classes))
        if random.random() < 0.5
        else parent2.get("consecuente", random.choice(classes))
    )

    # Sanea la regla por si faltara algo
    child = ensure_valid_rule(child, attributes, classes, variables)
    return child

def mutacion_regla(rule, variables, mutation_prob, attributes, classes):
    """
    Mutación por atributo con prob = mutation_prob. Nunca muta el consecuente.
    Luego sanea para asegurar consistencia.
    """
    mutated = dict(rule)
    for attr in attributes:
        if random.random() < mutation_prob:
            mutated[attr] = random.choice(_terms_for_attr(variables, attr))
    # (opcional) pequeña prob de mutar también el consecuente:
    # if random.random() < (mutation_prob / 2):
    #     mutated["consecuente"] = random.choice(classes)

    mutated = ensure_valid_rule(mutated, attributes, classes, variables)
    return mutated






# ---------- GA principal ----------

def algoritmo_genetico(
    train_data,
    test_data,
    col_clase,
    variables,
    num_rules=20,
    num_iter=10,
    TQ=30,
    cross_prob=0.9
):
    """
    Algoritmo genético para optimizar reglas y q en el enfoque Michigan.

    Retorna:
    - P_best: población final optimizada
    - q_best_used: vector q óptimo (tras la adaptación 1/(2-q) cuando aplica)
    - historial: lista de tuplas (iteración, CR_mejor_global, q_best_used)
    """
    # Atributos y clases
    attributes = [c for c in train_data.columns if c != col_clase]
    classes = list(train_data[col_clase].unique())
    n_clases = len(classes)

    # ---------- Paso 1: Generar población inicial P0 ----------
    rg = RuleGenerator(train_data, attributes, variables, N=num_rules, H=2)
    P0 = rg.generar_reglas(col_clase)
    # Sanear toda la población inicial
    P0 = [ensure_valid_rule(r, attributes, classes, variables) for r in P0]

    # ---------- Paso 2: Inicializar q y calcular CR inicial ----------
    solidez = SolidezChoquet(variables)
    q_best = [1.0] * n_clases                 # q "crudo"
    q_best_used = ajustar_q(q_best)      # q "adaptado" que realmente se usa
    CR_best = calcular_CR(test_data, P0, solidez, q_best_used, col_clase)
    P_best = deepcopy(P0)

    historial = [(0, CR_best, q_best_used.copy())]
    print(f"Iteración inicial: CR = {CR_best:.4f}, q_used = {q_best_used}")

    # ---------- Paso 3: Optimización inicial de q ----------
    for t in range(1, TQ + 1):
        q_random = np.random.uniform(0.01, 1.99, size=n_clases).tolist()  # q "crudo"
        q_random_used = ajustar_q(q_random)                           # q "usado"
        CR_t = calcular_CR(test_data, P0, solidez, q_random_used, col_clase)
        if CR_t > CR_best:
            q_best = q_random
            q_best_used = q_random_used
            CR_best = CR_t
            P_best = deepcopy(P0)
            print(f"TQ {t}: Nuevo mejor CR = {CR_best:.4f}, q_used = {q_best_used}")
    historial.append(("TQ", CR_best, q_best_used.copy()))

    # ---------- Paso 4: Evolución de la población ----------
    fitness_model = FitnessSimple(variables)

    for iteration in range(1, num_iter + 1):
        print(f"\n--- Iteración {iteration} ---")

        # Copiar la mejor población
        P_iter = deepcopy(P_best)

        # Calcular fitness de cada regla (para escoger peores)
        fitness_dict = fitness_model.calcular_fitness_todas(P_iter, train_data, col_clase)

        # Seleccionar las Nreplace peores reglas
        Nreplace = max(1, len(P_iter) // 2)
        peor_indices = sorted(fitness_dict, key=fitness_dict.get)[:Nreplace]

        # Generar nuevas reglas
        nuevas_reglas = []

        # 1) Reglas por operadores genéticos
        n_genetic = Nreplace // 2
        for _ in range(n_genetic):
            parent1, parent2 = random.sample(P_iter, 2)
            child = crossover_reglas(parent1, parent2, cross_prob, attributes, classes, variables)
            child = mutacion_regla(child, variables, mutation_prob=1/len(attributes), attributes=attributes, classes=classes)
            nuevas_reglas.append(child)

        # 2) Reglas por MPB (usando ejemplos mal clasificados)
        n_mpb = Nreplace - n_genetic
        misclassified = obtener_mal_clasificados(train_data, P_iter, solidez, q_best_used, col_clase)
        for _ in range(n_mpb):
            if not misclassified.empty:
                ejemplo_base = misclassified.sample(1).iloc[0]
                new_rule = regla_desde_ejemplo(ejemplo_base, variables, attributes, col_clase)
            else:
                # fallback: usar generador estándar
                new_rule, _ = rg.generar_regla(col_clase)
            new_rule = ensure_valid_rule(new_rule, attributes, classes, variables)
            nuevas_reglas.append(new_rule)

        # Reemplazar peores reglas
        for idx, new_rule in zip(peor_indices, nuevas_reglas):
            P_iter[idx] = new_rule

        # --- Evaluar población actual con q_best_used ---
        CR_iter = calcular_CR(test_data, P_iter, solidez, q_best_used, col_clase)
        print(f"CR con q_best_used: {CR_iter:.4f}")

        # Si esta población ya mejora, actualiza mejor global
        if CR_iter > CR_best:
            CR_best = CR_iter
            P_best = deepcopy(P_iter)
            print(f"Iter {iteration}: ¡Nueva mejor Población! CR = {CR_best:.4f}")

        # ---- Optimizar q para esta población (TQ veces) ----
        for t in range(TQ):
            q_random = np.random.uniform(0.01, 1.99, size=n_clases).tolist()  # crudo
            q_random_used = ajustar_q(q_random)                           # usado
            CR_t = calcular_CR(test_data, P_iter, solidez, q_random_used, col_clase)
            if CR_t > CR_best:
                q_best = q_random
                q_best_used = q_random_used
                CR_best = CR_t
                P_best = deepcopy(P_iter)
                print(f"Iter {iteration} - TQ {t}: Mejor CR = {CR_best:.4f}, q_used = {q_best_used}")

        historial.append((iteration, CR_best, q_best_used.copy()))

    print(f"\nMejor CR final: {CR_best:.4f}")
    print(f"Mejor q_used encontrado: {q_best_used}")
    return P_best, q_best_used, historial

# ---------- Aux para MPB(Multipatron Based) ----------

def obtener_mal_clasificados(data, reglas, solidez_model, q_vector_used, col_clase):
    """
    Retorna DataFrame con ejemplos mal clasificados por 'reglas' usando 'q_vector_used'.
    """
    mal = []
    for _, ejemplo in data.iterrows():
        solidez_por_clase = solidez_model.solidez_con_choquet(
            ejemplo=ejemplo,
            reglas=reglas,
            data=data,
            col_clase=col_clase,
            q_vector=q_vector_used
        )
        if len(solidez_por_clase) == 0:
            continue
        pred = max(solidez_por_clase, key=solidez_por_clase.get)
        if pred != ejemplo[col_clase]:
            mal.append(ejemplo)
    return pd.DataFrame(mal)




